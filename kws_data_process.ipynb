{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# path = '../vakyansh-wav2vec2-experimentation/data/pretraining/data_thchs30/data/A2_0.wav'\n",
    "# w1,s1 = torchaudio.load(path)\n",
    "# w2,s2 = librosa.load(path, sr=16000)\n",
    "# print(w1,w1.shape,s1)\n",
    "# print(w2,w2.shape,s2)\n",
    "\n",
    "# mel1 = librosa.feature.melspectrogram(y=w1.squeeze(0).numpy(), sr=16000, n_fft=640, n_mels=128,\n",
    "#                                                         hop_length=320, win_length=640)\n",
    "# mel2 = librosa.feature.melspectrogram(y=w2, sr=16000, n_fft=640, n_mels=128,\n",
    "#                                                         hop_length=320, win_length=640)\n",
    "# print(np.all(mel1==mel2))\n",
    "\n",
    "# # mel = mel.transpose(1, 0)\n",
    "# # print(mel)\n",
    "# # mel = librosa.power_to_db(mel)\n",
    "# # print(mel)\n",
    "# # mel = librosa.util.normalize(mel)\n",
    "# # print(mel)\n",
    "\n",
    "# # plt.imshow(mel)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../vakyansh-wav2vec2-experimentation/data/pretraining/data_thchs30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../vakyansh-wav2vec2-experimentation/data/pretraining/data_thchs30'\n",
    "lm_phone_folder, lm_phone_file = 'lm_phone', 'lexicon.txt'\n",
    "train_folder, valid_folder, test_folder, data_folder = 'train', 'dev', 'test', 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(folder, lm_phone_folder, lm_phone_file)\n",
    "VOCAB = []\n",
    "with open(file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        phone = line.split()[0]\n",
    "        VOCAB.append(phone)\n",
    "print(len(VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2token(s, vocab):\n",
    "    return vocab.index(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_length = 640\n",
    "hop_length = 320\n",
    "n_mels = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_folder = test_folder\n",
    "files = os.listdir(os.path.join(folder, wav_folder))\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "from model import Wav2Vec2Finetuner\n",
    "model = Wav2Vec2Finetuner(None, 219)\n",
    "inputs_length = []\n",
    "for file in files:\n",
    "    # For every .wav, get name\n",
    "    if file.endswith('wav'):\n",
    "        name = file.split('.')[0]\n",
    "        \n",
    "        wav_path = os.path.join(folder, wav_folder, file)\n",
    "        with open(wav_path, 'r') as f:\n",
    "            # waveform: tensor (1, frames)\n",
    "            waveform, sample_rate = torchaudio.load(wav_path)\n",
    "            inputs.append(waveform)\n",
    "            \n",
    "            out, _ = model(waveform, 0)\n",
    "            inputs_length.append(out.shape[1])\n",
    "            \n",
    "            # mel: ndarray (1, n_wins, n_mels)\n",
    "            # mel = librosa.feature.melspectrogram(y=waveform.numpy(), sr=sample_rate, n_fft=win_length, n_mels=n_mels,\n",
    "            #                                     hop_length=hop_length, win_length=win_length)\n",
    "            # mel = mel.transpose(0, 2, 1)\n",
    "            # mel = librosa.power_to_db(mel)\n",
    "            # mel = librosa.util.normalize(mel)\n",
    "            # inputs.append(mel)\n",
    "        \n",
    "        label_path = os.path.join(folder, data_folder, f\"{name}.wav.trn\")\n",
    "        with open(label_path, 'r') as f:\n",
    "            # Need <eps> explicitly or not?\n",
    "            # labels: ndarray (n_labels)\n",
    "            label_seq = np.array([str2token(s, VOCAB) for s in f.readlines()[2].strip().split()], dtype=int)\n",
    "            labels.append(label_seq)\n",
    "\n",
    "# inputs_length = []\n",
    "# # max_inputs_length = max([i.shape[1] for i in inputs])\n",
    "# # print(max_inputs_length)\n",
    "# for i, ipt in enumerate(inputs):\n",
    "#     inputs_length.append(ipt.shape[1])\n",
    "#     # to_pad = max_inputs_length - ipt.shape[1]\n",
    "#     # inputs[i] = np.pad(ipt, ((0,0),(0,to_pad)), mode='constant', constant_values=0)\n",
    "\n",
    "labels_length = []\n",
    "# max_labels_length = max([len(l) for l in labels])\n",
    "# print(max_labels_length)\n",
    "for i, lbl in enumerate(labels):\n",
    "    labels_length.append(lbl.shape[0])\n",
    "    # to_pad = max_labels_length - lbl.shape[0]\n",
    "    # labels[i] = np.expand_dims(np.pad(lbl, ((0,to_pad)), mode='constant', constant_values=0), 0)\n",
    "\n",
    "print(len(labels))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(inputs_length[i])\n",
    "    np.savez(f\"./Dataset/raw/test/{i:>04}.npz\", input=inputs[i].squeeze().numpy(), input_length=inputs_length[i], label=labels[i], label_length=labels_length[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(labels)):\n",
    "    np.savez(f\"./Dataset/raw/train/{i:>04}.npz\", input=inputs[i].squeeze().numpy(), input_length=inputs_length[i], label=labels[i], label_length=labels_length[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 (1, 250000) (1, 250000)\n",
      "10000 124800\n",
      "10000 (1, 94) (1, 94)\n",
      "10000 60\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs), inputs[0].shape, inputs[-1].shape)\n",
    "print(len(inputs_length), inputs_length[0])\n",
    "print(len(labels), labels[0].shape, labels[-1].shape)\n",
    "print(len(labels_length), labels_length[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate(inputs)\n",
    "inputs_length = np.array(inputs_length, dtype=int)\n",
    "labels = np.concatenate(labels)\n",
    "labels_length = np.array(labels_length, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(inputs), inputs[0].shape, np.concatenate(inputs).shape)\n",
    "# print(np.array(inputs_length, dtype=int))\n",
    "# print(len(labels), labels[0].shape, np.concatenate(labels).shape)\n",
    "# print(np.array(labels_length, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('./Dataset/raw/train.npz', inputs=inputs, inputs_length=inputs_length, labels=labels, labels_length=labels_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Dataset/raw/VOCAB.npy', VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 250000)\n",
      "(10000,)\n",
      "(10000, 94)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "ret = np.load('./Dataset/raw/train.npz')\n",
    "print(ret['inputs'].shape)\n",
    "print(ret['inputs_length'].shape)\n",
    "print(ret['labels'].shape)\n",
    "print(ret['labels_length'].shape)\n",
    "# np.load('./Dataset/VOCAB.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "data = np.load(f'./Dataset/raw/{split}.npz')\n",
    "inputs = data['inputs']\n",
    "inputs_length = data['inputs_length']\n",
    "labels = data['labels']\n",
    "labels_length = data['labels_length']\n",
    "for i in range(len(labels)):\n",
    "    np.savez(f\"./Dataset/{split}/{i:>04}.npz\", input=inputs[i], input_length=inputs_length[i], label=labels[i], label_length=labels_length[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n"
     ]
    }
   ],
   "source": [
    "with open('./Dataset/valid/len.txt', 'r') as f:\n",
    "    len = int(f.readline())\n",
    "print(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.plugins.io import TorchCheckpointIO as tcio\n",
    "from model import CTCEncoderDecoder\n",
    "from plmodel import LightningCTC\n",
    "\n",
    "config = {\n",
    "        'n_mels': 128,\n",
    "        'n_fft': 640,\n",
    "        'win_length': 640,\n",
    "        'hop_length': 320,\n",
    "        'wav_max_length': 782,\n",
    "        'transcript_max_length': 94,\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 64,\n",
    "        'weight_decay': 0,\n",
    "        'encoder_num_layers': 4,\n",
    "        'encoder_hidden_dim': 256,\n",
    "        'encoder_bidirectional': True,\n",
    "    }\n",
    "\n",
    "tc = tcio()\n",
    "ckpt_dict = tc.load_checkpoint('./Checkpoints/LSTM_test/epoch=77-step=12168.ckpt')\n",
    "print(ckpt_dict.keys())\n",
    "plmodel = LightningCTC(**config)\n",
    "plmodel.load_state_dict(ckpt_dict['state_dict'])\n",
    "model = plmodel.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
